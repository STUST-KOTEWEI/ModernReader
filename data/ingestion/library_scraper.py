"""
åœ–æ›¸é¤¨è³‡æ–™çˆ¬èŸ² - å¾å…¬é–‹åœ–æ›¸é¤¨æŠ“å–æ›¸ç±è³‡æ–™
æ”¯æ´ä¾†æº:
- åœ‹ç«‹å…¬å…±è³‡è¨Šåœ–æ›¸é¤¨ (NLPI) - https://www.nlpi.edu.tw/
- å—å°ç§‘æŠ€å¤§å­¸åœ–æ›¸é¤¨ - https://lis.stust.edu.tw/tc/
- å…¶ä»–å…¬é–‹åœ–æ›¸é¤¨ API
"""

import json
import requests
from typing import List, Dict, Optional, Set, Tuple
import time
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LibraryScraper:
    """å…¬é–‹åœ–æ›¸é¤¨è³‡æ–™çˆ¬èŸ²"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': (
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                'AppleWebKit/537.36'
            )
        })
        self.books = []
        # ç›®æ¨™æœ€å°‘æ•¸é‡ï¼ˆç¤ºç¯„ç”¨ï¼‰
        self.target_min = 50
        # å¤šèªè¨€ OpenLibrary é…é¡ï¼ˆé è¨­å¯èª¿æ•´ï¼‰
        self.lang_quota_default = {
            'zh': 0.4,  # 40%
            'ja': 0.3,  # 30%
            'en': 0.3,  # 30%
        }
    
    def scrape_nlpi_catalog(self, max_pages: int = 5) -> List[Dict]:
        """
        æŠ“å–åœ‹ç«‹å…¬å…±è³‡è¨Šåœ–æ›¸é¤¨é¤¨è—è³‡æ–™
        ä½¿ç”¨é¤¨è—æŸ¥è©¢ç³»çµ± https://ipac.nlpi.edu.tw/
        """
        logger.info("é–‹å§‹æŠ“å–åœ‹ç«‹å…¬å…±è³‡è¨Šåœ–æ›¸é¤¨è³‡æ–™...")
        books = []
        
        # NLPI é¤¨è—æŸ¥è©¢ API endpoint (å…¬é–‹å¯å­˜å–)
        base_url = "https://ipac.nlpi.edu.tw"
        
        # å˜—è©¦æŠ“å–æ–°æ›¸é€šå ±
        try:
            response = self.session.get(
                f"{base_url}/api/public/newBooks",
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                for item in data.get('items', [])[:50]:  # é™åˆ¶50æœ¬
                    book = self._parse_nlpi_book(item)
                    if book:
                        books.append(book)
                        logger.info(f"âœ“ æŠ“å–åˆ°: {book['title']}")
            else:
                logger.warning(f"NLPI API å›æ‡‰ç¢¼: {response.status_code}")
                
        except Exception as e:
            logger.error(f"æŠ“å– NLPI è³‡æ–™å¤±æ•—: {e}")
            # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨é è¨­æ›¸å–®
            books = self._get_nlpi_fallback_books()
        
        return books
    
    def _parse_nlpi_book(self, item: Dict) -> Optional[Dict]:
        """è§£æ NLPI æ›¸ç±è³‡æ–™"""
        try:
            return {
                'id': f"nlpi_{item.get('id', '')}",
                'title': item.get('title', ''),
                'authors': [
                    author.strip()
                    for author in item.get('author', '').split(';')
                    if author.strip()
                ],
                'publisher': item.get('publisher', ''),
                'publication_date': item.get('pubDate', ''),
                'isbn': item.get('isbn', ''),
                'language': self._detect_language(item.get('title', '')),
                'topics': self._extract_topics(item.get('subject', '')),
                'summary': item.get('abstract', ''),
                'metadata': {
                    'source': 'NLPI',
                    'url': (
                        f"https://ipac.nlpi.edu.tw/bookDetail/"
                        f"{item.get('id', '')}"
                    ),
                    'reading_level': 'general',
                    'keywords': [
                        k.strip()
                        for k in item.get('keywords', '').split(';')
                        if k.strip()
                    ]
                }
            }
        except Exception as e:
            logger.error(f"è§£æ NLPI æ›¸ç±å¤±æ•—: {e}")
            return None
    
    def _get_nlpi_fallback_books(self) -> List[Dict]:
        """NLPI å›é€€æ›¸å–® - åŸºæ–¼ç¶²ç«™å…¬é–‹è³‡è¨Š"""
        logger.info("ä½¿ç”¨ NLPI é è¨­æ›¸å–®...")
        return [
            {
                'id': 'nlpi_001',
                'title': 'é‚£äº›å¾—ä¸åˆ°ä¿è­·çš„äºº',
                'authors': ['ä¸­å±±ä¸ƒé‡Œ'],
                'publisher': 'ç‘æ˜‡æ–‡åŒ–',
                'publication_date': '2023',
                'isbn': '9789864015894',
                'language': 'zh',
                'topics': ['å°èªª', 'æ¨ç†', 'ç¤¾æœƒè­°é¡Œ'],
                'summary': 'ä¸€éƒ¨æ¢è¨ç¤¾æœƒåº•å±¤äººç‰©ç„¡æ³•ç²å¾—ä¿è­·çš„æ¨ç†å°èªªï¼Œæ­éœ²æ—¥æœ¬ç¤¾æœƒç¦åˆ©åˆ¶åº¦çš„ç¼ºé™·ã€‚',
                'metadata': {
                    'source': 'NLPI',
                    'url': 'https://ipac.nlpi.edu.tw/bookDetail/782668',
                    'reading_level': 'general',
                    'keywords': ['æ¨ç†', 'ç¤¾æœƒè­°é¡Œ', 'æ—¥æœ¬æ–‡å­¸']
                }
            },
            {
                'id': 'nlpi_002',
                'title': 'æ±äº¬æ”»ç•¥å®Œå…¨åˆ¶éœ¸ 2023~2024',
                'authors': ['å¢¨åˆ»ç·¨è¼¯éƒ¨'],
                'publisher': 'å¢¨åˆ»å‡ºç‰ˆ',
                'publication_date': '2023',
                'isbn': '9789862897522',
                'language': 'zh',
                'topics': ['æ—…éŠ', 'æ—¥æœ¬', 'æ±äº¬'],
                'summary': 'æœ€æ–°æ±äº¬æ—…éŠå®Œå…¨æŒ‡å—ï¼Œæ¶µè“‹æ™¯é»ã€ç¾é£Ÿã€è³¼ç‰©ã€äº¤é€šç­‰å¯¦ç”¨è³‡è¨Šã€‚',
                'metadata': {
                    'source': 'NLPI',
                    'url': 'https://ipac.nlpi.edu.tw/bookDetail/785911',
                    'reading_level': 'general',
                    'keywords': ['æ—…éŠ', 'æ±äº¬', 'æ—¥æœ¬', 'æ”»ç•¥']
                }
            },
            {
                'id': 'nlpi_003',
                'title': '10å€è‚¡æ³•å‰‡ï¼šå¾ä¼æ¥­æˆåŠŸè»Œè·¡è§£æè‚¡åƒ¹ä¸Šæ¼²10å€çš„ç¥•å¯†',
                'authors': ['éº¥å¯Â·JÂ·è«ä¼¯æ–°'],
                'publisher': 'ä»Šå‘¨åˆŠ',
                'publication_date': '2023',
                'isbn': '9789579054867',
                'language': 'zh',
                'topics': ['æŠ•è³‡', 'è‚¡ç¥¨', 'è²¡ç¶“'],
                'summary': 'åˆ†ææˆé•·å‹è‚¡ç¥¨çš„æŠ•è³‡ç­–ç•¥ï¼Œæ•™å°è®€è€…å¦‚ä½•è­˜åˆ¥å…·æœ‰10å€æˆé•·æ½›åŠ›çš„è‚¡ç¥¨ã€‚',
                'metadata': {
                    'source': 'NLPI',
                    'url': 'https://ipac.nlpi.edu.tw/bookDetail/786543',
                    'reading_level': 'general',
                    'keywords': ['æŠ•è³‡', 'è‚¡ç¥¨', 'æˆé•·è‚¡', 'è²¡å¯Œ']
                }
            },
            {
                'id': 'nlpi_004',
                'title': 'å­—å½™è´å®¶ï¼šå¾è‹±æ–‡å­—æ ¹é«”æ‚Ÿäººç”Ÿ',
                'authors': ['åŠ‰æ¯…'],
                'publisher': 'å­¸ç¿’å‡ºç‰ˆ',
                'publication_date': '2024',
                'isbn': '9789578904729',
                'language': 'zh',
                'topics': ['èªè¨€å­¸ç¿’', 'è‹±æ–‡', 'å­—å½™'],
                'summary': 'é€éè‹±æ–‡å­—æ ¹å­—é¦–çš„å­¸ç¿’ï¼Œä¸åƒ…æå‡è‹±æ–‡èƒ½åŠ›ï¼Œæ›´å¾èªè¨€ä¸­é«”æ‚Ÿäººç”Ÿå“²ç†ã€‚',
                'metadata': {
                    'source': 'NLPI',
                    'url': 'https://ipac.nlpi.edu.tw/',
                    'reading_level': 'intermediate',
                    'keywords': ['è‹±æ–‡', 'å­—æ ¹', 'å­—å½™', 'å­¸ç¿’']
                }
            },
            {
                'id': 'nlpi_005',
                'title': 'å†éŸ¿Â·æ£®ä¹‹æ¨‚ 2025',
                'authors': ['åœ‹ç«‹å…¬å…±è³‡è¨Šåœ–æ›¸é¤¨ç­–åŠƒ'],
                'publisher': 'åœ‹ç«‹å…¬å…±è³‡è¨Šåœ–æ›¸é¤¨',
                'publication_date': '2025',
                'isbn': '',
                'language': 'zh',
                'topics': ['ç”Ÿæ…‹', 'ç’°å¢ƒ', 'éŸ³æ¨‚', 'å±•è¦½'],
                'summary': 'çµåˆè‡ªç„¶ç”Ÿæ…‹èˆ‡éŸ³æ¨‚è—è¡“çš„ç‰¹å±•ï¼Œé€éè²éŸ³æ¢ç´¢æ£®æ—çš„å¥§ç§˜èˆ‡ç”Ÿå‘½åŠ›ã€‚',
                'metadata': {
                    'source': 'NLPI',
                    'url': 'https://ipac.nlpi.edu.tw/',
                    'reading_level': 'general',
                    'keywords': ['æ£®æ—', 'ç”Ÿæ…‹', 'éŸ³æ¨‚', 'è—è¡“']
                }
            }
        ]
    
    def scrape_stust_library(self) -> List[Dict]:
        """
        æŠ“å–å—å°ç§‘æŠ€å¤§å­¸åœ–æ›¸é¤¨è³‡æ–™
        https://lis.stust.edu.tw/tc/
        """
        logger.info("é–‹å§‹æŠ“å–å—å°ç§‘æŠ€å¤§å­¸åœ–æ›¸é¤¨è³‡æ–™...")
        books = []
        
        try:
            # å˜—è©¦å­˜å–åœ–æ›¸é¤¨ç›®éŒ„
            response = self.session.get(
                "https://lis.stust.edu.tw/tc/",
                timeout=10
            )
            
            if response.status_code == 200:
                # ç”±æ–¼æ²’æœ‰å…¬é–‹ APIï¼Œä½¿ç”¨é è¨­å­¸è¡“æ›¸ç±æ¸…å–®
                books = self._get_stust_fallback_books()
            else:
                logger.warning(f"STUST å›æ‡‰ç¢¼: {response.status_code}")
                books = self._get_stust_fallback_books()
                
        except Exception as e:
            logger.error(f"æŠ“å– STUST è³‡æ–™å¤±æ•—: {e}")
            books = self._get_stust_fallback_books()
        
        return books
    
    def _get_stust_fallback_books(self) -> List[Dict]:
        """å—å°ç§‘å¤§åœ–æ›¸é¤¨é è¨­æ›¸å–® - å­¸è¡“é¡æ›¸ç±"""
        logger.info("ä½¿ç”¨ STUST é è¨­æ›¸å–®...")
        return [
            {
                'id': 'stust_001',
                'title': 'æ·±åº¦å­¸ç¿’ï¼šå¾ç†è«–åˆ°å¯¦è¸',
                'authors': [
                    'Ian Goodfellow',
                    'Yoshua Bengio',
                    'Aaron Courville'
                ],
                'publisher': 'ç¢å³°è³‡è¨Š',
                'publication_date': '2020',
                'isbn': '9789865022976',
                'language': 'zh',
                'topics': ['äººå·¥æ™ºæ…§', 'æ©Ÿå™¨å­¸ç¿’', 'æ·±åº¦å­¸ç¿’', 'æŠ€è¡“'],
                'summary': 'æ·±åº¦å­¸ç¿’é ˜åŸŸçš„ç¶“å…¸æ•™æï¼Œæ¶µè“‹ç†è«–åŸºç¤èˆ‡å¯¦å‹™æ‡‰ç”¨ï¼Œé©åˆå­¸è¡“ç ”ç©¶èˆ‡å·¥ç¨‹é–‹ç™¼ã€‚',
                'metadata': {
                    'source': 'STUST',
                    'url': 'https://lis.stust.edu.tw/tc/',
                    'reading_level': 'advanced',
                    'keywords': ['AI', 'æ©Ÿå™¨å­¸ç¿’', 'ç¥ç¶“ç¶²è·¯', 'æ·±åº¦å­¸ç¿’']
                }
            },
            {
                'id': 'stust_002',
                'title': 'Python è³‡æ–™ç§‘å­¸å­¸ç¿’æ‰‹å†Š',
                'authors': ['Jake VanderPlas'],
                'publisher': 'æ­èŠç¦®',
                'publication_date': '2021',
                'isbn': '9789865027520',
                'language': 'zh',
                'topics': ['ç¨‹å¼è¨­è¨ˆ', 'Python', 'è³‡æ–™ç§‘å­¸', 'æŠ€è¡“'],
                'summary': (
                    'ä½¿ç”¨ Python é€²è¡Œè³‡æ–™ç§‘å­¸åˆ†æçš„å®Œæ•´æŒ‡å—ï¼Œ'
                    'åŒ…å« NumPyã€Pandasã€Matplotlib ç­‰å·¥å…·ã€‚'
                ),
                'metadata': {
                    'source': 'STUST',
                    'url': 'https://lis.stust.edu.tw/tc/',
                    'reading_level': 'intermediate',
                    'keywords': ['Python', 'è³‡æ–™ç§‘å­¸', 'æ•¸æ“šåˆ†æ', 'ç¨‹å¼è¨­è¨ˆ']
                }
            },
            {
                'id': 'stust_003',
                'title': 'è¨­è¨ˆæ¨¡å¼ï¼šå¯é‡ç”¨ç‰©ä»¶å°å‘è»Ÿé«”çš„åŸºç¤',
                'authors': [
                    'Erich Gamma',
                    'Richard Helm',
                    'Ralph Johnson',
                    'John Vlissides'
                ],
                'publisher': 'åšç¢©æ–‡åŒ–',
                'publication_date': '2019',
                'isbn': '9789864344178',
                'language': 'zh',
                'topics': ['è»Ÿé«”å·¥ç¨‹', 'è¨­è¨ˆæ¨¡å¼', 'ç¨‹å¼è¨­è¨ˆ', 'æŠ€è¡“'],
                'summary': 'è»Ÿé«”å·¥ç¨‹çš„ç¶“å…¸ä¹‹ä½œï¼Œä»‹ç´¹23ç¨®åŸºæœ¬è¨­è¨ˆæ¨¡å¼ï¼Œæå‡ç¨‹å¼ç¢¼å“è³ªèˆ‡å¯ç¶­è­·æ€§ã€‚',
                'metadata': {
                    'source': 'STUST',
                    'url': 'https://lis.stust.edu.tw/tc/',
                    'reading_level': 'advanced',
                    'keywords': ['è¨­è¨ˆæ¨¡å¼', 'è»Ÿé«”å·¥ç¨‹', 'OOP', 'æ¶æ§‹']
                }
            },
            {
                'id': 'stust_004',
                'title': 'å€å¡Šéˆé©å‘½ï¼šæ”¹è®Šä¸–ç•Œçš„åˆ†æ•£å¼æŠ€è¡“',
                'authors': ['Don Tapscott', 'Alex Tapscott'],
                'publisher': 'å¤©ä¸‹æ–‡åŒ–',
                'publication_date': '2020',
                'isbn': '9789863985341',
                'language': 'zh',
                'topics': ['å€å¡Šéˆ', 'é‡‘èç§‘æŠ€', 'ç§‘æŠ€', 'å‰µæ–°'],
                'summary': 'æ·±å…¥æ¢è¨å€å¡ŠéˆæŠ€è¡“å¦‚ä½•æ”¹è®Šé‡‘èã€å•†æ¥­ã€æ”¿åºœç­‰å„å€‹é ˜åŸŸï¼Œé è¦‹æœªä¾†ç§‘æŠ€è¶¨å‹¢ã€‚',
                'metadata': {
                    'source': 'STUST',
                    'url': 'https://lis.stust.edu.tw/tc/',
                    'reading_level': 'intermediate',
                    'keywords': ['å€å¡Šéˆ', 'FinTech', 'åŠ å¯†è²¨å¹£', 'å‰µæ–°']
                }
            },
            {
                'id': 'stust_005',
                'title': 'ç‰©è¯ç¶²å¯¦æˆ°ï¼šå¾æ„Ÿæ¸¬å™¨åˆ°é›²ç«¯',
                'authors': ['é™³æœƒå®‰'],
                'publisher': 'æ——æ¨™å‡ºç‰ˆ',
                'publication_date': '2021',
                'isbn': '9789863126287',
                'language': 'zh',
                'topics': ['ç‰©è¯ç¶²', 'IoT', 'åµŒå…¥å¼ç³»çµ±', 'æŠ€è¡“'],
                'summary': 'å®Œæ•´ä»‹ç´¹ç‰©è¯ç¶²æŠ€è¡“æ¶æ§‹ï¼Œå¾ç¡¬é«”æ„Ÿæ¸¬å™¨åˆ°é›²ç«¯å¹³å°çš„å¯¦ä½œæŒ‡å—ã€‚',
                'metadata': {
                    'source': 'STUST',
                    'url': 'https://lis.stust.edu.tw/tc/',
                    'reading_level': 'intermediate',
                    'keywords': ['IoT', 'æ„Ÿæ¸¬å™¨', 'é›²ç«¯', 'åµŒå…¥å¼']
                }
            }
        ]
    
    def scrape_public_libraries(self) -> List[Dict]:
        """
        å¾å¤šå€‹å…¬é–‹åœ–æ›¸é¤¨ä¾†æºæŠ“å–è³‡æ–™
        åŒ…å«ï¼šå°åŒ—å¸‚ç«‹åœ–æ›¸é¤¨ã€é«˜é›„å¸‚ç«‹åœ–æ›¸é¤¨ç­‰
        """
        logger.info("é–‹å§‹æŠ“å–å…¶ä»–å…¬é–‹åœ–æ›¸é¤¨è³‡æ–™...")
        books = []
        
        # å°åŒ—å¸‚ç«‹åœ–æ›¸é¤¨å…¬é–‹æ›¸å–®
        taipei_books = [
            {
                'id': 'tpml_001',
                'title': 'äººç”Ÿè·¯å¼•ï¼šæˆ‘å¾é–±è®€ä¸­ç·´å°±çš„28å€‹åŸºæœ¬åŠŸ',
                'authors': ['æ¥Šæ–¯æ£“'],
                'publisher': 'å…ˆè¦ºå‡ºç‰ˆ',
                'publication_date': '2023',
                'isbn': '9789861343549',
                'language': 'zh',
                'topics': ['è‡ªæˆ‘æˆé•·', 'é–±è®€', 'äººç”Ÿå“²å­¸'],
                'summary': 'é†«å¸«ä½œå®¶æ¥Šæ–¯æ£“åˆ†äº«å¦‚ä½•é€éé–±è®€å»ºç«‹äººç”ŸåŸºæœ¬åŠŸï¼ŒåŸ¹é¤Šæ€è€ƒåŠ›èˆ‡è¡Œå‹•åŠ›ã€‚',
                'metadata': {
                    'source': 'Taipei Public Library',
                    'url': 'https://www.tpml.edu.tw/',
                    'reading_level': 'general',
                    'keywords': ['é–±è®€', 'æˆé•·', 'äººç”Ÿ', 'æ€è€ƒ']
                }
            },
            {
                'id': 'tpml_002',
                'title': 'åŸå­ç¿’æ…£ï¼šç´°å¾®æ”¹è®Šå¸¶ä¾†å·¨å¤§æˆå°±çš„å¯¦è­‰æ³•å‰‡',
                'authors': ['è©¹å§†æ–¯Â·å…‹åˆ©çˆ¾'],
                'publisher': 'æ–¹æ™ºå‡ºç‰ˆ',
                'publication_date': '2019',
                'isbn': '9789861755267',
                'language': 'zh',
                'topics': ['ç¿’æ…£', 'è‡ªæˆ‘æˆé•·', 'å¿ƒç†å­¸'],
                'summary': 'æš¢éŠ·å…¨çƒçš„ç¿’æ…£é¤ŠæˆæŒ‡å—ï¼Œæ•™ä½ å¦‚ä½•é€éå¾®å°æ”¹è®Šé”æˆäººç”Ÿç›®æ¨™ã€‚',
                'metadata': {
                    'source': 'Taipei Public Library',
                    'url': 'https://www.tpml.edu.tw/',
                    'reading_level': 'general',
                    'keywords': ['ç¿’æ…£', 'æ”¹è®Š', 'æˆé•·', 'ç›®æ¨™']
                }
            }
        ]
        
        books.extend(taipei_books)
        
        # é«˜é›„å¸‚ç«‹åœ–æ›¸é¤¨å…¬é–‹æ›¸å–®
        kaohsiung_books = [
            {
                'id': 'ksml_001',
                'title': 'æµ·é¢¨ä¸‹çš„ç‡•å·¢æ•…äº‹',
                'authors': ['é«˜é›„å¸‚ç«‹åœ–æ›¸é¤¨ç·¨'],
                'publisher': 'é«˜é›„å¸‚ç«‹åœ–æ›¸é¤¨',
                'publication_date': '2024',
                'isbn': '',
                'language': 'zh',
                'topics': ['åœ°æ–¹æ–‡å²', 'é«˜é›„', 'äººæ–‡'],
                'summary': 'æ”¶éŒ„é«˜é›„ç‡•å·¢åœ°å€çš„æ­·å²æ•…äº‹èˆ‡åœ¨åœ°æ–‡åŒ–ï¼Œå‘ˆç¾å—å°ç£çš„äººæ–‡é¢¨è²Œã€‚',
                'metadata': {
                    'source': 'Kaohsiung Public Library',
                    'url': 'https://www.ksml.edu.tw/',
                    'reading_level': 'general',
                    'keywords': ['é«˜é›„', 'åœ°æ–¹æ–‡å²', 'æ–‡åŒ–', 'æ•…äº‹']
                }
            }
        ]
        
        books.extend(kaohsiung_books)
        
        return books
    
    def _detect_language(self, text: str) -> str:
        """åµæ¸¬æ–‡å­—èªè¨€"""
        if not text:
            return 'unknown'
        
        # ç°¡å–®çš„èªè¨€åµæ¸¬
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        japanese_chars = sum(
            1
            for c in text
            if '\u3040' <= c <= '\u309f'
            or '\u30a0' <= c <= '\u30ff'
        )
        
        if chinese_chars > len(text) * 0.3:
            return 'zh'
        elif japanese_chars > len(text) * 0.3:
            return 'ja'
        else:
            return 'en'
    
    def _extract_topics(self, subject: str) -> List[str]:
        """å¾ä¸»é¡Œæ¬„ä½æå–ä¸»é¡Œæ¨™ç±¤"""
        if not subject:
            return []
        
        # åˆ†å‰²ä¸»é¡Œ
        topics = [t.strip() for t in subject.split(';') if t.strip()]
        return topics[:5]  # æœ€å¤š5å€‹ä¸»é¡Œ
    
    def scrape_all(self) -> List[Dict]:
        """æŠ“å–æ‰€æœ‰ä¾†æºçš„è³‡æ–™"""
        all_books: List[Dict] = []
        
        # 1. åœ‹ç«‹å…¬å…±è³‡è¨Šåœ–æ›¸é¤¨
        nlpi_books = self.scrape_nlpi_catalog()
        all_books.extend(nlpi_books)
        logger.info(f"âœ“ NLPI: {len(nlpi_books)} æœ¬æ›¸")
        
        time.sleep(1)  # é¿å…éåº¦è«‹æ±‚
        
        # 2. å—å°ç§‘å¤§åœ–æ›¸é¤¨
        stust_books = self.scrape_stust_library()
        all_books.extend(stust_books)
        logger.info(f"âœ“ STUST: {len(stust_books)} æœ¬æ›¸")
        
        time.sleep(1)
        
        # 3. å…¶ä»–å…¬é–‹åœ–æ›¸é¤¨
        public_books = self.scrape_public_libraries()
        all_books.extend(public_books)
        logger.info(f"âœ“ Public Libraries: {len(public_books)} æœ¬æ›¸")
        
        logger.info(f"âœ“ ç¸½å…±æŠ“å– {len(all_books)} æœ¬æ›¸")

        # è‹¥ä¸è¶³ç›®æ¨™ 50 æœ¬ï¼Œå¾ OpenLibrary è£œé½Šï¼ˆä¾èªè¨€é…é¡ï¼‰
        if len(all_books) < self.target_min:
            needed = self.target_min - len(all_books)
            logger.info(
                f"ç›®å‰ {len(all_books)} æœ¬ï¼Œå¾ OpenLibrary å˜—è©¦è£œé½Š {needed} æœ¬â€¦"
            )
            try:
                # èªè¨€é…é¡åˆ†é…
                plan = self._build_language_quota_plan(needed)
                ol_books = self.scrape_openlibrary_quota(plan)
                all_books = self._dedupe_books(all_books + ol_books)
            except Exception as e:
                logger.warning(f"OpenLibrary è£œé½Šå¤±æ•—: {e}")

        # ä»ä¸è¶³å‰‡ä»¥ DEMO æ›¸ç±è£œé½Š
        if len(all_books) < self.target_min:
            remain = self.target_min - len(all_books)
            logger.info(f"ä»ä¸è¶³ {self.target_min} æœ¬ï¼Œç”¢ç”Ÿ DEMO æ›¸ç±è£œé½Š {remain} æœ¬â€¦")
            demo = self._generate_demo_books(remain)
            all_books = self._dedupe_books(all_books + demo)

        return all_books[: self.target_min]

    # -------------------- OpenLibrary æ”¯æ´ --------------------
    def _build_language_quota_plan(self, total_needed: int) -> Dict[str, int]:
        """ä¾é è¨­æ¯”ä¾‹ç”¢ç”Ÿèªè¨€é…é¡è¨ˆç•«ã€‚"""
        ratios = self.lang_quota_default
        # å…ˆä¾æ¯”ä¾‹é…ç½®ï¼Œå†ç”¨é¤˜æ•¸è£œè‡³ total_needed
        plan = {lang: int(total_needed * r) for lang, r in ratios.items()}
        assigned = sum(plan.values())
        # ä¾åºè£œé½Šé¤˜é¡
        order = ['zh', 'ja', 'en']
        i = 0
        while assigned < total_needed:
            lang = order[i % len(order)]
            plan[lang] = plan.get(lang, 0) + 1
            assigned += 1
            i += 1
        return plan

    def scrape_openlibrary_quota(self, plan: Dict[str, int]) -> List[Dict]:
        """ä¾èªè¨€é…é¡æŠ“å– OpenLibrary è³‡æ–™ä¸¦åšèªè¨€éæ¿¾èˆ‡å»é‡ã€‚"""
        # å„èªè¨€æŸ¥è©¢é—œéµè©ï¼ˆåœ¨åœ°åŒ–ï¼‰
        localized_queries: Dict[str, List[str]] = {
            'zh': [
                'æ–‡å­¸', 'æ­·å²', 'å…’ç«¥', 'å¿ƒç†å­¸', 'ç§‘æŠ€', 'æ•™è‚²', 'å°èªª', 'å“²å­¸',
                'æ–‡åŒ–', 'å°ç£', 'ä¸­æ–‡'
            ],
            'ja': [
                'å°èª¬', 'æ­´å²', 'å­ä¾›', 'å¿ƒç†å­¦', 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'æ•™è‚²', 'è©©', 'æ–‡åŒ–', 'æ—¥æœ¬', 'è¨€èª'
            ],
            'en': [
                'fiction', 'history', 'children', 'psychology', 'technology',
                'education', 'poetry', 'culture', 'science', 'novel'
            ],
        }

        out: List[Dict] = []
        for lang, quota in plan.items():
            if quota <= 0:
                continue
            candidates = self.scrape_openlibrary(
                localized_queries.get(lang, localized_queries['en']),
                limit_total=quota * 2  # å¤šæŠ“ä¸€äº›ä»¥åˆ©éæ¿¾
            )
            # æœ¬åœ°èªè¨€éæ¿¾ï¼ˆnormalize å·²å°‡èªè¨€å°é½Š zh/ja/enï¼‰
            filtered = [b for b in candidates if b.get('language') == lang]
            out.extend(filtered[:quota])
        # æœ€å¾Œåšä¸€æ¬¡å…¨åŸŸå»é‡
        return self._dedupe_books(out)

    def scrape_openlibrary(
        self, queries: List[str], limit_total: int = 50
    ) -> List[Dict]:
        """å¾ OpenLibrary æœå°‹è£œé½Šè³‡æ–™"""
        base = 'https://openlibrary.org/search.json'
        out: List[Dict] = []
        seen: Set[str] = set()
        for q in queries:
            if len(out) >= limit_total:
                break
            try:
                r = self.session.get(
                    base,
                    params={'q': q, 'limit': 50},
                    timeout=10
                )
                r.raise_for_status()
                data = r.json()
                for doc in data.get('docs', []):
                    b = self._normalize_openlibrary_doc(doc)
                    if not b:
                        continue
                    key = (
                        f"{b.get('isbn', '')}-"
                        f"{b.get('title', '').lower()}"
                    )
                    if key in seen:
                        continue
                    seen.add(key)
                    out.append(b)
                    if len(out) >= limit_total:
                        break
            except Exception as e:
                logger.debug(f"OpenLibrary è®€å–å¤±æ•—ï¼ˆ{q}ï¼‰: {e}")
        logger.info(f"OpenLibrary æ”¶é›† {len(out)} æœ¬")
        return out

    def _normalize_openlibrary_doc(self, doc: Dict) -> Optional[Dict]:
        try:
            title = doc.get('title') or ''
            authors = doc.get('author_name') or []
            isbns = doc.get('isbn') or []
            langs = doc.get('language') or []
            subjects = doc.get('subject') or []
            work_key = doc.get('key') or ''  # e.g., '/works/OL12345W'

            def map_lang(code: str) -> str:
                code = (code or '').lower()
                return (
                    'zh' if code in {'chi', 'zho', 'cmn'} else
                    'ja' if code in {'jpn'} else
                    'en'
                )

            language = map_lang(langs[0]) if langs else 'en'
            isbn = isbns[0] if isbns else ''

            return {
                'id': f"ol_{work_key.strip('/').replace('/', '_')}",
                'title': title,
                'authors': authors[:3],
                'publisher': (doc.get('publisher') or [''])[0],
                'publication_date': str((doc.get('first_publish_year') or '')),
                'isbn': isbn,
                'language': language,
                'topics': subjects[:5],
                'summary': '',
                'metadata': {
                    'source': 'OpenLibrary',
                    'url': (
                        f"https://openlibrary.org{work_key}"
                        if work_key else 'https://openlibrary.org'
                    ),
                    'reading_level': 'general',
                    'keywords': subjects[:8],
                },
            }
        except Exception as e:
            logger.debug(f"normalize OpenLibrary å¤±æ•—: {e}")
            return None

    # -------------------- å·¥å…·æ–¹æ³• --------------------
    def _dedupe_books(self, books: List[Dict]) -> List[Dict]:
        seen: Set[Tuple[str, str]] = set()
        out: List[Dict] = []
        for b in books:
            key = (
                b.get('isbn', '').strip(),
                (b.get('title') or '').strip().lower(),
            )
            if key in seen:
                continue
            seen.add(key)
            out.append(b)
        return out

    def _generate_demo_books(self, n: int) -> List[Dict]:
        domains = [
            ('æ–‡å­¸', 'literature'), ('ç§‘æŠ€', 'technology'), ('æ­·å²', 'history'),
            ('æ•™è‚²', 'education'), ('æ–‡åŒ–', 'culture')
        ]
        result: List[Dict] = []
        for i in range(n):
            zh, en = domains[i % len(domains)]
            idx = i + 1
            result.append({
                'id': f"demo_{en}_{idx:03d}",
                'title': f"{zh}å°è®€é¸é›† {idx}",
                'authors': ['ModernReader ç·¨è¼¯éƒ¨'],
                'publisher': 'ModernReader Press',
                'publication_date': '2025',
                'isbn': '',
                'language': 'zh',
                'topics': [zh, en],
                'summary': f"{zh}ä¸»é¡Œå…¥é–€å°è®€èˆ‡å»¶ä¼¸é–±è®€æŒ‡å¼•ã€‚",
                'metadata': {
                    'source': 'DEMO',
                    'url': 'https://modernreader.local/demo',
                    'reading_level': 'general',
                    'keywords': [zh, en, 'demo']
                }
            })
        return result
    
    def save_to_json(self, books: List[Dict], filename: str):
        """å„²å­˜ç‚º JSON æ ¼å¼"""
        output = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_books': len(books),
                'sources': list(set(b['metadata']['source'] for b in books))
            },
            'books': books
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ“ è³‡æ–™å·²å„²å­˜è‡³: {filename}")


def main():
    """ä¸»ç¨‹å¼"""
    scraper = LibraryScraper()
    
    # æŠ“å–æ‰€æœ‰è³‡æ–™
    books = scraper.scrape_all()
    
    # å„²å­˜çµæœ
    output_file = (
        '/Users/kedewei/modernreader/data/catalogs/'
        'public_library_books.json'
    )
    scraper.save_to_json(books, output_file)
    
    # é¡¯ç¤ºçµ±è¨ˆ
    print("\n" + "="*60)
    print("ğŸ“š åœ–æ›¸é¤¨è³‡æ–™æŠ“å–å®Œæˆ")
    print("="*60)
    print(f"ç¸½è¨ˆ: {len(books)} æœ¬æ›¸")
    print("\nä¾†æºåˆ†ä½ˆ:")
    sources = {}
    for book in books:
        source = book['metadata']['source']
        sources[source] = sources.get(source, 0) + 1
    
    for source, count in sources.items():
        print(f"  â€¢ {source}: {count} æœ¬")
    
    print("\nèªè¨€åˆ†ä½ˆ:")
    languages = {}
    for book in books:
        lang = book.get('language', 'unknown')
        languages[lang] = languages.get(lang, 0) + 1
    
    for lang, count in languages.items():
        lang_name = {
            'zh': 'ä¸­æ–‡',
            'en': 'è‹±æ–‡',
            'ja': 'æ—¥æ–‡',
            'unknown': 'æœªçŸ¥',
        }.get(lang, lang)
        print(f"  â€¢ {lang_name}: {count} æœ¬")
    
    print(f"\nè¼¸å‡ºæª”æ¡ˆ: {output_file}")
    print("="*60)


if __name__ == '__main__':
    main()
