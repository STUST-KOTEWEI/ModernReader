/**
 * AI Model Manager
 * Manages multiple Gemini models with fallback strategy
 * Priority: 2.5 Pro â†’ 2.5 Flash â†’ 2.0 Pro â†’ 2.0 Flash
 * Optimized for MacBook Air M3 8GB RAM
 */

import { GoogleGenerativeAI } from '@google/generative-ai';

interface ModelConfig {
  name: string;
  displayName: string;
  priority: number;
  maxTokens: number;
  costPerToken: number;
  available: boolean;
  usageCount: number;
  errorCount: number;
  lastUsed: number;
  rateLimit: {
    requestsPerMinute: number;
    tokensPerMinute: number;
  };
}

interface GenerationRequest {
  prompt: string;
  temperature?: number;
  maxTokens?: number;
  preferredModel?: string;
  systemInstruction?: string;
}

interface GenerationResponse {
  text: string;
  model: string;
  tokensUsed: number;
  processingTime: number;
}

class AIModelManager {
  private genAI: GoogleGenerativeAI;
  private models: Map<string, ModelConfig> = new Map();
  private currentModel: string = 'gemini-2.5-pro';
  private requestQueue: Map<string, number[]> = new Map();

  constructor() {
    const apiKey = import.meta.env.VITE_GEMINI_API_KEY;
    this.genAI = new GoogleGenerativeAI(apiKey);
    this.initializeModels();
    this.startHealthMonitoring();
  }

  /**
   * åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨æ¨¡å‹
   */
  private initializeModels(): void {
    // Gemini 2.5 Pro - æœ€å¼·å¤§ï¼Œå„ªå…ˆä½¿ç”¨
    this.models.set('gemini-2.5-pro', {
      name: 'gemini-2.5-pro',
      displayName: 'Gemini 2.5 Pro',
      priority: 1,
      maxTokens: 1048576, // 1M tokens
      costPerToken: 0.00025,
      available: true,
      usageCount: 0,
      errorCount: 0,
      lastUsed: 0,
      rateLimit: {
        requestsPerMinute: 10,
        tokensPerMinute: 100000,
      },
    });

    // Gemini 2.5 Flash - å¿«é€Ÿä¸”ç¶“æ¿Ÿ
    this.models.set('gemini-2.5-flash', {
      name: 'gemini-2.5-flash',
      displayName: 'Gemini 2.5 Flash',
      priority: 2,
      maxTokens: 1048576,
      costPerToken: 0.00005,
      available: true,
      usageCount: 0,
      errorCount: 0,
      lastUsed: 0,
      rateLimit: {
        requestsPerMinute: 15,
        tokensPerMinute: 150000,
      },
    });

    // Gemini 2.0 Pro - å‚™ç”¨é¸é …
    this.models.set('gemini-2.0-pro', {
      name: 'gemini-2.0-pro',
      displayName: 'Gemini 2.0 Pro',
      priority: 3,
      maxTokens: 32768,
      costPerToken: 0.0002,
      available: true,
      usageCount: 0,
      errorCount: 0,
      lastUsed: 0,
      rateLimit: {
        requestsPerMinute: 12,
        tokensPerMinute: 120000,
      },
    });

    // Gemini 2.0 Flash - æœ€å¾Œå‚™ç”¨
    this.models.set('gemini-2.0-flash', {
      name: 'gemini-2.0-flash',
      displayName: 'Gemini 2.0 Flash',
      priority: 4,
      maxTokens: 8192,
      costPerToken: 0.00003,
      available: true,
      usageCount: 0,
      errorCount: 0,
      lastUsed: 0,
      rateLimit: {
        requestsPerMinute: 20,
        tokensPerMinute: 200000,
      },
    });

    console.log('ğŸ¤– AI Model Manager å·²åˆå§‹åŒ–ï¼Œå¯ç”¨æ¨¡å‹:', Array.from(this.models.keys()));
  }

  /**
   * æ™ºèƒ½ç”Ÿæˆæ–‡æœ¬ï¼ˆè‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹ï¼‰
   */
  async generate(request: GenerationRequest): Promise<GenerationResponse> {
    const startTime = Date.now();
    
    // é¸æ“‡æ¨¡å‹
    const modelName = await this.selectBestModel(request);
    
    try {
      const model = this.genAI.getGenerativeModel({ model: modelName });
      
      // é…ç½®ç”Ÿæˆåƒæ•¸
      const generationConfig = {
        temperature: request.temperature || 0.7,
        maxOutputTokens: request.maxTokens || 2048,
      };

      // ç”Ÿæˆå…§å®¹
      const result = await model.generateContent({
        contents: [{ role: 'user', parts: [{ text: request.prompt }] }],
        generationConfig,
        systemInstruction: request.systemInstruction,
      });

      const response = result.response;
      const text = response.text();
      
      // æ›´æ–°çµ±è¨ˆ
      const modelConfig = this.models.get(modelName)!;
      modelConfig.usageCount++;
      modelConfig.lastUsed = Date.now();
      
      // è¨˜éŒ„è«‹æ±‚æ™‚é–“ï¼ˆç”¨æ–¼é€Ÿç‡é™åˆ¶ï¼‰
      this.recordRequest(modelName);

      const processingTime = Date.now() - startTime;

      console.log(`âœ… ${modelConfig.displayName} ç”ŸæˆæˆåŠŸ (${processingTime}ms)`);

      return {
        text,
        model: modelName,
        tokensUsed: this.estimateTokens(request.prompt + text),
        processingTime,
      };

    } catch (error) {
      console.error(`âŒ ${modelName} ç”Ÿæˆå¤±æ•—:`, error);
      
      // æ¨™è¨˜æ¨¡å‹éŒ¯èª¤
      const modelConfig = this.models.get(modelName);
      if (modelConfig) {
        modelConfig.errorCount++;
        modelConfig.available = modelConfig.errorCount < 3; // 3æ¬¡éŒ¯èª¤å¾Œç¦ç”¨
      }

      // å˜—è©¦å‚™ç”¨æ¨¡å‹
      return this.fallbackGenerate(request, modelName);
    }
  }

  /**
   * é¸æ“‡æœ€ä½³æ¨¡å‹
   */
  private async selectBestModel(request: GenerationRequest): Promise<string> {
    // å¦‚æœæŒ‡å®šäº†æ¨¡å‹ä¸”å¯ç”¨ï¼Œä½¿ç”¨æŒ‡å®šæ¨¡å‹
    if (request.preferredModel && this.models.has(request.preferredModel)) {
      const model = this.models.get(request.preferredModel)!;
      if (model.available && !this.isRateLimited(request.preferredModel)) {
        return request.preferredModel;
      }
    }

    // æŒ‰å„ªå…ˆé †åºé¸æ“‡å¯ç”¨æ¨¡å‹
    const availableModels = Array.from(this.models.values())
      .filter(m => m.available && !this.isRateLimited(m.name))
      .sort((a, b) => a.priority - b.priority);

    if (availableModels.length === 0) {
      console.warn('âš ï¸ æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨æˆ–é”åˆ°é€Ÿç‡é™åˆ¶ï¼Œä½¿ç”¨é è¨­æ¨¡å‹');
      return 'gemini-2.0-flash';
    }

    const selectedModel = availableModels[0];
    console.log(`ğŸ¯ é¸æ“‡æ¨¡å‹: ${selectedModel.displayName} (å„ªå…ˆç´š ${selectedModel.priority})`);
    
    return selectedModel.name;
  }

  /**
   * å‚™ç”¨æ¨¡å‹ç”Ÿæˆ
   */
  private async fallbackGenerate(
    request: GenerationRequest,
    failedModel: string
  ): Promise<GenerationResponse> {
    console.log(`ğŸ”„ åˆ‡æ›åˆ°å‚™ç”¨æ¨¡å‹...`);

    // ç²å–ä¸‹ä¸€å€‹å¯ç”¨æ¨¡å‹
    const availableModels = Array.from(this.models.values())
      .filter(m => m.name !== failedModel && m.available && !this.isRateLimited(m.name))
      .sort((a, b) => a.priority - b.priority);

    if (availableModels.length === 0) {
      throw new Error('æ‰€æœ‰ AI æ¨¡å‹éƒ½ä¸å¯ç”¨');
    }

    // ä½¿ç”¨ä¸‹ä¸€å€‹æ¨¡å‹
    request.preferredModel = availableModels[0].name;
    return this.generate(request);
  }

  /**
   * æª¢æŸ¥æ˜¯å¦é”åˆ°é€Ÿç‡é™åˆ¶
   */
  private isRateLimited(modelName: string): boolean {
    const model = this.models.get(modelName);
    if (!model) return true;

    const requests = this.requestQueue.get(modelName) || [];
    const now = Date.now();
    const recentRequests = requests.filter(time => now - time < 60000); // æœ€è¿‘ 1 åˆ†é˜

    return recentRequests.length >= model.rateLimit.requestsPerMinute;
  }

  /**
   * è¨˜éŒ„è«‹æ±‚ï¼ˆç”¨æ–¼é€Ÿç‡é™åˆ¶ï¼‰
   */
  private recordRequest(modelName: string): void {
    if (!this.requestQueue.has(modelName)) {
      this.requestQueue.set(modelName, []);
    }
    
    const requests = this.requestQueue.get(modelName)!;
    requests.push(Date.now());

    // æ¸…ç†èˆŠè¨˜éŒ„ï¼ˆè¶…é 1 åˆ†é˜ï¼‰
    const now = Date.now();
    this.requestQueue.set(
      modelName,
      requests.filter(time => now - time < 60000)
    );
  }

  /**
   * ä¼°è¨ˆ Token æ•¸é‡
   */
  private estimateTokens(text: string): number {
    // ç°¡å–®ä¼°è¨ˆï¼šè‹±æ–‡ ~4 å­—ç¬¦/tokenï¼Œä¸­æ–‡ ~2 å­—ç¬¦/token
    const chineseChars = (text.match(/[\u4e00-\u9fa5]/g) || []).length;
    const otherChars = text.length - chineseChars;
    return Math.ceil(chineseChars / 2 + otherChars / 4);
  }

  /**
   * ç²å–æ¨¡å‹ç‹€æ…‹
   */
  getModelStatus(): ModelConfig[] {
    return Array.from(this.models.values()).sort((a, b) => a.priority - b.priority);
  }

  /**
   * ç²å–ä½¿ç”¨çµ±è¨ˆ
   */
  getUsageStats(): {
    totalRequests: number;
    totalErrors: number;
    modelBreakdown: Record<string, { requests: number; errors: number; successRate: number }>;
    estimatedCost: number;
  } {
    let totalRequests = 0;
    let totalErrors = 0;
    let estimatedCost = 0;
    const modelBreakdown: Record<string, any> = {};

    this.models.forEach((config, name) => {
      totalRequests += config.usageCount;
      totalErrors += config.errorCount;
      
      const successRate = config.usageCount > 0
        ? ((config.usageCount - config.errorCount) / config.usageCount) * 100
        : 0;

      modelBreakdown[name] = {
        requests: config.usageCount,
        errors: config.errorCount,
        successRate: successRate.toFixed(2) + '%',
      };

      // ä¼°è¨ˆæˆæœ¬ï¼ˆå‡è¨­å¹³å‡ 1000 tokens/è«‹æ±‚ï¼‰
      estimatedCost += config.usageCount * 1000 * config.costPerToken;
    });

    return {
      totalRequests,
      totalErrors,
      modelBreakdown,
      estimatedCost: Math.round(estimatedCost * 100) / 100,
    };
  }

  /**
   * é‡ç½®æ¨¡å‹éŒ¯èª¤è¨ˆæ•¸
   */
  resetModelErrors(modelName?: string): void {
    if (modelName && this.models.has(modelName)) {
      const model = this.models.get(modelName)!;
      model.errorCount = 0;
      model.available = true;
      console.log(`âœ… ${model.displayName} éŒ¯èª¤å·²é‡ç½®`);
    } else {
      this.models.forEach(model => {
        model.errorCount = 0;
        model.available = true;
      });
      console.log('âœ… æ‰€æœ‰æ¨¡å‹éŒ¯èª¤å·²é‡ç½®');
    }
  }

  /**
   * å¥åº·ç›£æ§
   */
  private startHealthMonitoring(): void {
    setInterval(() => {
      const stats = this.getUsageStats();
      
      // å¦‚æœéŒ¯èª¤ç‡éé«˜ï¼Œé‡ç½®éŒ¯èª¤è¨ˆæ•¸
      this.models.forEach(model => {
        if (model.errorCount >= 3 && Date.now() - model.lastUsed > 300000) {
          // 5åˆ†é˜æœªä½¿ç”¨ï¼Œé‡ç½®éŒ¯èª¤
          model.errorCount = 0;
          model.available = true;
          console.log(`ğŸ”„ ${model.displayName} å·²è‡ªå‹•æ¢å¾©`);
        }
      });

      console.log('ğŸ“Š AI æ¨¡å‹å¥åº·ç‹€æ…‹:', {
        ç¸½è«‹æ±‚: stats.totalRequests,
        ç¸½éŒ¯èª¤: stats.totalErrors,
        ä¼°è¨ˆæˆæœ¬: `$${stats.estimatedCost}`,
      });
    }, 300000); // æ¯ 5 åˆ†é˜æª¢æŸ¥
  }

  /**
   * æ‰¹æ¬¡ç”Ÿæˆï¼ˆä¸¦è¡Œè™•ç†å¤šå€‹è«‹æ±‚ï¼‰
   */
  async batchGenerate(requests: GenerationRequest[]): Promise<GenerationResponse[]> {
    console.log(`ğŸš€ æ‰¹æ¬¡ç”Ÿæˆ ${requests.length} å€‹è«‹æ±‚...`);
    
    // é™åˆ¶ä¸¦è¡Œæ•¸é‡ä»¥é¿å…è¶…å‡ºé€Ÿç‡é™åˆ¶
    const batchSize = 3;
    const results: GenerationResponse[] = [];

    for (let i = 0; i < requests.length; i += batchSize) {
      const batch = requests.slice(i, i + batchSize);
      const batchResults = await Promise.all(
        batch.map(req => this.generate(req))
      );
      results.push(...batchResults);

      // æ‰¹æ¬¡ä¹‹é–“ç¨ä½œå»¶é²
      if (i + batchSize < requests.length) {
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    }

    console.log(`âœ… æ‰¹æ¬¡ç”Ÿæˆå®Œæˆï¼Œå…± ${results.length} å€‹çµæœ`);
    return results;
  }

  /**
   * ä¸²æµç”Ÿæˆï¼ˆé©åˆé•·æ–‡æœ¬ï¼‰
   */
  async *generateStream(request: GenerationRequest): AsyncGenerator<string, void, unknown> {
    const modelName = await this.selectBestModel(request);
    const model = this.genAI.getGenerativeModel({ model: modelName });

    const generationConfig = {
      temperature: request.temperature || 0.7,
      maxOutputTokens: request.maxTokens || 2048,
    };

    const result = await model.generateContentStream({
      contents: [{ role: 'user', parts: [{ text: request.prompt }] }],
      generationConfig,
      systemInstruction: request.systemInstruction,
    });

    console.log(`ğŸŒŠ é–‹å§‹ä¸²æµç”Ÿæˆ (${modelName})...`);

    for await (const chunk of result.stream) {
      const text = chunk.text();
      yield text;
    }

    console.log('âœ… ä¸²æµç”Ÿæˆå®Œæˆ');
  }
}

export const aiModelManager = new AIModelManager();
export default aiModelManager;
